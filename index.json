[{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"","permalink":"//localhost:1313/posts/ai-cost/","summary":"","title":""},{"content":"Explosion of crawling\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eExplosion of crawling\u003c/p\u003e\n\u003ch2 id=\"the-cost-of-ai-in-travel-distribution\"\u003eThe Cost of AI in Travel Distribution\u003c/h2\u003e\n\u003cp\u003eThe cost of AI in travel distribution is variable and depends on the use case.\u003c/p\u003e\n\u003ch2 id=\"the-cost-of-ai-in-travel-distribution-1\"\u003eThe Cost of AI in Travel Distribution\u003c/h2\u003e\n\u003cp\u003eThe cost of AI in travel distribution is variable and depends on the use case.\u003c/p\u003e","title":"The Cost of AI in Travel"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"Explosion of crawling and RAG usage is driving up costs.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eExplosion of crawling and RAG usage is driving up costs.\u003c/p\u003e\n\u003ch2 id=\"the-cost-of-ai-in-travel-distribution\"\u003eThe Cost of AI in Travel Distribution\u003c/h2\u003e\n\u003cp\u003eThe cost of AI in travel distribution is variable and depends on the use case.\u003c/p\u003e\n\u003ch2 id=\"the-cost-of-ai-in-travel-distribution-1\"\u003eThe Cost of AI in Travel Distribution\u003c/h2\u003e\n\u003cp\u003eThe cost of AI in travel distribution is variable and depends on the use case.\u003c/p\u003e","title":"The Cost of AI in Travel"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"Explosion of crawling and RAG usage is driving up costs.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eExplosion of crawling and RAG usage is driving up costs.\u003c/p\u003e\n\u003ch2 id=\"the-cost-of-ai-in-travel-distribution\"\u003eThe Cost of AI in Travel Distribution\u003c/h2\u003e\n\u003cp\u003eThe cost of AI in travel distribution is variable and depends on the use case.\u003c/p\u003e\n\u003ch2 id=\"the-cost-of-ai-in-travel-distribution-1\"\u003eThe Cost of AI in Travel Distribution\u003c/h2\u003e\n\u003cp\u003eThe cost of AI in travel distribution is variable and depends on the use case.\u003c/p\u003e","title":"Why are you paying for other people's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"No AI system capable of handling availability search itself, they all require using APIs to get the data.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eNo AI system capable of handling availability search itself, they all require using APIs to get the data.\u003c/p\u003e\n\u003ch2 id=\"the-cost-of-ai-in-travel-distribution\"\u003eThe Cost of AI in Travel Distribution\u003c/h2\u003e\n\u003cp\u003eThe cost of AI in travel distribution is variable and depends on the use case.\u003c/p\u003e\n\u003ch2 id=\"the-cost-of-ai-in-travel-distribution-1\"\u003eThe Cost of AI in Travel Distribution\u003c/h2\u003e\n\u003cp\u003eThe cost of AI in travel distribution is variable and depends on the use case.\u003c/p\u003e","title":"Why are you paying for other people's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We all seen the demos, new applications. User talks to an AI agent and their vacation is planned by the AI. But what is going on in the background?\nFact: No AI system capable of handling availability search by itself, it requires processing thousands of hotel availability records to generate a quote. They all require using APIs to get the data.\nWelcome to RAG: Retriever-Augmented Generation. AI agent makes multiple requests to the API to get relevant data to answer the user\u0026rsquo;s question. Which means a search request that normally would cost $0.01 now costs $0.05 because of the additional requests.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe all seen the demos, new applications. User talks to an AI agent and their vacation is planned by the AI. But what is going on in the background?\u003c/p\u003e\n\u003cp\u003eFact: No AI system capable of handling availability search by itself, it requires processing thousands of hotel availability records to generate a quote. They all require using APIs to get the data.\u003c/p\u003e\n\u003cp\u003eWelcome to RAG: Retriever-Augmented Generation.\nAI agent makes multiple requests to the API to get relevant data to answer the user\u0026rsquo;s question. Which means a search request that normally would cost $0.01 now costs $0.05 because of the additional requests.\u003c/p\u003e","title":"Why are you paying for other people's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We all seen the demos, new applications. User talks to an AI agent and their vacation is planned by the AI. But what is going on in the background?\nFact: No AI system capable of handling availability search by itself, it requires processing thousands of hotel availability records to generate a quote. They all require using booking engine APIs to get the data.\nWelcome to RAG: Retriever-Augmented Generation. AI agent makes multiple requests to the API to get relevant data to answer the user\u0026rsquo;s question. Which means a search request that normally would cost $0.01 now costs $0.05 because of the additional requests.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\nThe Cost of AI in Travel Distribution The cost of AI in travel distribution is variable and depends on the use case.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe all seen the demos, new applications. User talks to an AI agent and their vacation is planned by the AI. But what is going on in the background?\u003c/p\u003e\n\u003cp\u003eFact: No AI system capable of handling availability search by itself, it requires processing thousands of hotel availability records to generate a quote. They all require using booking engine APIs to get the data.\u003c/p\u003e\n\u003cp\u003eWelcome to RAG: Retriever-Augmented Generation.\nAI agent makes multiple requests to the API to get relevant data to answer the user\u0026rsquo;s question. Which means a search request that normally would cost $0.01 now costs $0.05 because of the additional requests.\u003c/p\u003e","title":"Bedbanks are paying for other people's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We all seen the demos, new applications. User talks to an AI agent and their vacation is planned by the AI. But what is going on in the background?\nFact: No AI system capable of handling availability search by itself, it requires processing thousands of hotel availability records to generate a quote. They all require using booking engine APIs to get the data.\nWelcome to RAG: Retriever-Augmented Generation. AI agent makes multiple requests to the API to get relevant data to answer the user\u0026rsquo;s question. Which means a search request that normally would cost $0.01 now costs $0.05 because of the additional requests.\nThe cost of AI in travel distribution is variable and depends on the use case.\nBar to create crawlers is just lowered Even if your system is not using RAG, the bar to create crawlers is just lowered. Many companies offer \u0026ldquo;competitor analysis\u0026rdquo; as a service. Which means they will crawl your results and compare them to their own. This was being done before, but creating such products are easier therefore more companies are offering this service.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe all seen the demos, new applications. User talks to an AI agent and their vacation is planned by the AI. But what is going on in the background?\u003c/p\u003e\n\u003cp\u003eFact: No AI system capable of handling availability search by itself, it requires processing thousands of hotel availability records to generate a quote. They all require using booking engine APIs to get the data.\u003c/p\u003e\n\u003ch2 id=\"welcome-to-rag-retriever-augmented-generation\"\u003eWelcome to RAG: Retriever-Augmented Generation.\u003c/h2\u003e\n\u003cp\u003eAI agent makes multiple requests to the API to get relevant data to answer the user\u0026rsquo;s question. Which means a search request that normally would cost $0.01 now costs $0.05 because of the additional requests.\u003c/p\u003e","title":"Bedbanks are paying for other people's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We all seen the demos, new applications. User talks to an AI agent and their vacation is planned by the AI. But what is going on in the background?\nFact: No AI system capable of handling availability search by itself, it requires processing thousands of hotel availability records to generate a quote. They all require using booking engine APIs to get the data.\nWelcome to RAG: Retriever-Augmented Generation. AI agent makes multiple requests to the API to get relevant data to answer the user\u0026rsquo;s question. Which means a search request that normally would cost $0.01 now costs $0.05 because of the additional requests.\nThe cost of AI in travel distribution is variable and depends on the use case.\nBar to create crawlers is just lowered Even if your system is not using RAG, the bar to create crawlers is just lowered. Many companies offer \u0026ldquo;competitor analysis\u0026rdquo; as a service. Which means they will crawl your results and compare them to their own. This was being done before, but creating such products are easier therefore more companies are offering this service.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe all seen the demos, new applications. User talks to an AI agent and their vacation is planned by the AI. But what is going on in the background?\u003c/p\u003e\n\u003cp\u003eFact: No AI system capable of handling availability search by itself, it requires processing thousands of hotel availability records to generate a quote. They all require using booking engine APIs to get the data.\u003c/p\u003e\n\u003ch2 id=\"welcome-to-rag-retriever-augmented-generation\"\u003eWelcome to RAG: Retriever-Augmented Generation.\u003c/h2\u003e\n\u003cp\u003eAI agent makes multiple requests to the API to get relevant data to answer the user\u0026rsquo;s question. Which means a search request that normally would cost $0.01 now costs $0.05 because of the additional requests.\u003c/p\u003e","title":"Why Bedbanks are paying for other people's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We‚Äôve all seen the demos.\nA user chats with an AI agent and gets a perfectly planned vacation.\nThe interface is magical. Seamless. Effortless.\nBut here‚Äôs the real question:\nWhat is happening behind the scenes ‚Äî and who is paying for it?\nSpoiler: It‚Äôs not the AI company. It‚Äôs you.\nüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User There is a growing misconception in travel tech:\n‚ÄúAI will replace the booking engines.‚Äù\nNo.\nAI does zero availability computation.\nLLMs cannot evaluate millions of rate combinations or room contracts.\nWhat AI does is:\ninterpret user intent break it into multiple sub-questions call booking APIs repeatedly ask for more data until it can confidently answer This means AI replaces the user, not the travel stack.\nThe backend still does all the work ‚Äî just 5‚Äì10√ó more of it.\nüîç Welcome to RAG: Retriever-Augmented Generation When an AI agent tries to answer:\n‚ÄúShow me hotels in Barcelona under ‚Ç¨150 with a sea view and free cancellation.‚Äù\nIt doesn‚Äôt send one availability request.\nIt sends many:\nretrieve price buckets retrieve cancellation policies retrieve board types retrieve room-level metadata check availability again to confirm re-filter after user clarification repeat steps for multiple suppliers This is the RAG pipeline:\nRetrieve ‚Üí Reason ‚Üí Retrieve ‚Üí Answer.\nüìà The impact: A search that used to cost $0.01 now easily becomes $0.05‚Äì$0.10.\nMultiply by thousands of RAG calls per hour, per OTA, per partner‚Ä¶\nAnd suddenly your infrastructure cost explodes ‚Äî not the AI company‚Äôs.\nü§ñ AI Has Lowered the Bar for Crawlers Even if your platform is not using RAG internally, you‚Äôre still affected.\nWhy?\nBecause AI tools make screen scraping and crawling far easier.\nCompetitor intelligence startups can now spin up:\nprice crawlers rate-parity compliance bots availability testers optimization scripts content diff bots ‚Ä¶with almost no engineering.\nBefore LLMs, building these tools required real engineering talent.\nNow:\nGPT writes your crawler Cloudflare proxies it A $5/month server runs it A new ‚Äúcompetitive intelligence company‚Äù appears every week Your search API becomes the data source for dozens of third parties ‚Äî\nmany of whom you will never know about.\nResult: Your infrastructure is working harder than ever,\nfor people who are not even your customers.\nüí£ The Hidden Cost Explosion in Travel Distribution Let‚Äôs summarize the problem:\nAI agents multiply search traffic.\nOne user ‚Üí many backend calls.\nCrawlers and scrapers multiply traffic further.\nZero friction to build, cheap to run.\nLegacy booking engines were not designed for multi-agent LLM usage.\nThey choke under unpredictable load.\nSearch is already the most expensive part of travel distribution.\nAI turns an expensive operation into a very expensive one.\nMost systems pay $50‚Äì$100 per million searches.\nWith RAG, that becomes $300‚Äì$700.\nWho pays?\nBedbanks. Consolidators. OTAs. Suppliers.\nEveryone except the AI companies.\nüßµ The New Problem AI Creates (That Nobody Is Tal ","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe‚Äôve all seen the demos.\u003cbr\u003e\nA user chats with an AI agent and gets a perfectly planned vacation.\u003cbr\u003e\nThe interface is magical. Seamless. Effortless.\u003c/p\u003e\n\u003cp\u003eBut here‚Äôs the real question:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is happening behind the scenes ‚Äî and who is paying for it?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpoiler: \u003cem\u003eIt‚Äôs not the AI company. It‚Äôs you.\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-ai-is-not-replacing-software--its-replacing-the-user\"\u003eüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User\u003c/h2\u003e\n\u003cp\u003eThere is a growing misconception in travel tech:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúAI will replace the booking engines.‚Äù\u003c/p\u003e","title":"Why Bedbanks Are Paying For Other People's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We‚Äôve all seen the demos.\nA user chats with an AI agent and gets a perfectly planned vacation.\nThe interface is magical. Seamless. Effortless.\nBut here‚Äôs the real question:\nWhat is happening behind the scenes ‚Äî and who is paying for it?\nSpoiler: It‚Äôs not the AI company. It‚Äôs you.\nüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User There is a growing misconception in travel tech:\n‚ÄúAI will replace the booking engines.‚Äù\nNo.\nAI does zero availability computation.\nLLMs cannot evaluate millions of rate combinations or room contracts.\nWhat AI does is:\ninterpret user intent break it into multiple sub-questions call booking APIs repeatedly ask for more data until it can confidently answer This means AI replaces the user, not the travel stack.\nThe backend still does all the work ‚Äî just 5‚Äì10√ó more of it.\nüîç Welcome to RAG: Retriever-Augmented Generation When an AI agent tries to answer:\n‚ÄúShow me hotels in Barcelona under ‚Ç¨150 with a sea view and free cancellation.‚Äù\nIt doesn‚Äôt send one availability request.\nIt sends many:\nretrieve price buckets retrieve cancellation policies retrieve board types retrieve room-level metadata check availability again to confirm re-filter after user clarification repeat steps for multiple suppliers This is the RAG pipeline:\nRetrieve ‚Üí Reason ‚Üí Retrieve ‚Üí Answer.\nüìà The impact: A search that used to cost $0.01 now easily becomes $0.05‚Äì$0.10.\nMultiply by thousands of RAG calls per hour, per OTA, per partner‚Ä¶\nAnd suddenly your infrastructure cost explodes ‚Äî not the AI company‚Äôs.\nü§ñ AI Has Lowered the Bar for Crawlers Even if your platform is not using RAG internally, you‚Äôre still affected.\nWhy?\nBecause AI tools make screen scraping and crawling far easier.\nCompetitor intelligence startups can now spin up:\nprice crawlers rate-parity compliance bots availability testers optimization scripts content diff bots ‚Ä¶with almost no engineering.\nBefore LLMs, building these tools required real engineering talent.\nNow:\nGPT writes your crawler Cloudflare proxies it A $5/month server runs it A new ‚Äúcompetitive intelligence company‚Äù appears every week Your search API becomes the data source for dozens of third parties ‚Äî\nmany of whom you will never know about.\nResult: Your infrastructure is working harder than ever,\nfor people who are not even your customers.\nüí£ The Hidden Cost Explosion in Travel Distribution Let‚Äôs summarize the problem:\nAI agents multiply search traffic.\nOne user ‚Üí many backend calls.\nCrawlers and scrapers multiply traffic further.\nZero friction to build, cheap to run.\nLegacy booking engines were not designed for multi-agent LLM usage.\nThey choke under unpredictable load.\nSearch is already the most expensive part of travel distribution.\nAI turns an expensive operation into a very expensive one.\nMost systems pay $50‚Äì$100 per million searches.\nWith RAG, that becomes $300‚Äì$700.\nWho pays?\nBedbanks. Consolidators. OTAs. Suppliers.\nEveryone except the AI companies.\nüßµ The New Problem AI Creates (That Nobody Is Talking About) Travel is one of the few industries where:\nresults depend on real-time data prices change constantly availability shifts instantly suppliers and contracts are complex merging/dedupe is computationally heavy LLM interfaces don‚Äôt simplify this.\nThey stress the infrastructure harder.\nAI creates a backend problem ‚Äî not a UI problem. And this is exactly why efficient, low-cost, ultra-fast availability engines will decide which travel companies survive.\nüìå Final Thought AI looks magical.\nBut someone is paying the price for the magic.\nToday, that someone is the bedbank, wholesaler, or OTA running the availability API.\nSearch cost is becoming the biggest hidden expense in travel.\nAI is accelerating it.\nAnd the industry isn‚Äôt prepared.\nIn future posts, I will explain:\nwhy AI-era search load increases 5‚Äì10√ó how to reduce cost per million search by 100√ó how to build RAG-friendly availability caches and why most travel systems will break under AI load Stay tuned.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe‚Äôve all seen the demos.\u003cbr\u003e\nA user chats with an AI agent and gets a perfectly planned vacation.\u003cbr\u003e\nThe interface is magical. Seamless. Effortless.\u003c/p\u003e\n\u003cp\u003eBut here‚Äôs the real question:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is happening behind the scenes ‚Äî and who is paying for it?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpoiler: \u003cem\u003eIt‚Äôs not the AI company. It‚Äôs you.\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-ai-is-not-replacing-software--its-replacing-the-user\"\u003eüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User\u003c/h2\u003e\n\u003cp\u003eThere is a growing misconception in travel tech:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúAI will replace the booking engines.‚Äù\u003c/p\u003e","title":"Why Bedbanks Are Paying For Other People's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We‚Äôve all seen the demos.\nA user chats with an AI agent and gets a perfectly planned vacation.\nThe interface is magical. Seamless. Effortless.\nBut here‚Äôs the real question:\nWhat is happening behind the scenes ‚Äî and who is paying for it?\nSpoiler: It‚Äôs not the AI company. It‚Äôs you.\nüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User There is a growing misconception in travel tech:\n‚ÄúAI will replace the booking engines.‚Äù\nNo.\nAI does zero availability computation.\nLLMs cannot evaluate millions of rate combinations or room contracts.\nWhat AI does is:\ninterpret user intent break it into multiple sub-questions call booking APIs repeatedly ask for more data until it can confidently answer This means AI replaces the user, not the travel stack.\nThe backend still does all the work ‚Äî just 5‚Äì10√ó more of it.\nüîç Welcome to RAG: Retriever-Augmented Generation When an AI agent tries to answer:\n‚ÄúShow me hotels in Barcelona under ‚Ç¨150 with a sea view and free cancellation.‚Äù\nIt doesn‚Äôt send one availability request.\nIt sends many:\nretrieve price buckets retrieve cancellation policies retrieve board types retrieve room-level metadata check availability again to confirm re-filter after user clarification repeat steps for multiple suppliers This is the RAG pipeline:\nRetrieve ‚Üí Reason ‚Üí Retrieve ‚Üí Answer.\nüìà The impact: A search that used to cost $0.01 now easily becomes $0.05‚Äì$0.10.\nMultiply by thousands of RAG calls per hour, per OTA, per partner‚Ä¶\nAnd suddenly your infrastructure cost explodes ‚Äî not the AI company‚Äôs.\nü§ñ AI Has Lowered the Bar for Crawlers Even if your platform is not using RAG internally, you‚Äôre still affected.\nWhy?\nBecause AI tools make screen scraping and crawling far easier.\nCompetitor intelligence startups can now spin up:\nprice crawlers rate-parity compliance bots availability testers optimization scripts content diff bots ‚Ä¶with almost no engineering.\nBefore LLMs, building these tools required real engineering talent.\nNow:\nGPT writes your crawler Cloudflare proxies it A $5/month server runs it A new ‚Äúcompetitive intelligence company‚Äù appears every week Your search API becomes the data source for dozens of third parties ‚Äî\nmany of whom you will never know about.\nResult: Your infrastructure is working harder than ever,\nfor people who are not even your customers.\nüí£ The Hidden Cost Explosion in Travel Distribution Let‚Äôs summarize the problem:\nAI agents multiply search traffic.\nOne user ‚Üí many backend calls.\nCrawlers and scrapers multiply traffic further.\nZero friction to build, cheap to run.\nLegacy booking engines were not designed for multi-agent LLM usage.\nThey choke under unpredictable load.\nSearch is already the most expensive part of travel distribution.\nAI turns an expensive operation into a very expensive one.\nMost systems pay $50‚Äì$100 per million searches.\nWith RAG, that becomes $300‚Äì$700.\nWho pays?\nBedbanks. Consolidators. OTAs. Suppliers.\nEveryone except the AI companies.\nüßµ The New Problem AI Creates (That Nobody Is Talking About) Travel is one of the few industries where:\nresults depend on real-time data prices change constantly availability shifts instantly suppliers and contracts are complex merging/dedupe is computationally heavy LLM interfaces don‚Äôt simplify this.\nThey stress the infrastructure harder.\nAI creates a backend problem ‚Äî not a UI problem. And this is exactly why efficient, low-cost, ultra-fast availability engines will decide which travel companies survive.\nüìå Final Thought AI looks magical.\nBut someone is paying the price for the magic.\nToday, that someone is the bedbank, wholesaler, or OTA running the availability API.\nSearch cost is becoming the biggest hidden expense in travel.\nAI is accelerating it.\nAnd the industry isn‚Äôt prepared.\nIn future posts, I will explain:\nwhy AI-era search load increases 5‚Äì10√ó how to reduce cost per million search by 100√ó how to build RAG-friendly availability caches and why most travel systems will break under AI load Stay tuned.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe‚Äôve all seen the demos.\u003cbr\u003e\nA user chats with an AI agent and gets a perfectly planned vacation.\u003cbr\u003e\nThe interface is magical. Seamless. Effortless.\u003c/p\u003e\n\u003cp\u003eBut here‚Äôs the real question:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is happening behind the scenes ‚Äî and who is paying for it?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpoiler: \u003cem\u003eIt‚Äôs not the AI company. It‚Äôs you.\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-ai-is-not-replacing-software--its-replacing-the-user\"\u003eüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User\u003c/h2\u003e\n\u003cp\u003eThere is a growing misconception in travel tech:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúAI will replace the booking engines.‚Äù\u003c/p\u003e","title":"Why Bedbanks Are You Paying For Other People's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We‚Äôve all seen the demos.\nA user chats with an AI agent and gets a perfectly planned vacation.\nThe interface is magical. Seamless. Effortless.\nBut here‚Äôs the real question:\nWhat is happening behind the scenes ‚Äî and who is paying for it?\nSpoiler: It‚Äôs not the AI company. It‚Äôs you.\nüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User There is a growing misconception in travel tech:\n‚ÄúAI will replace the booking engines.‚Äù\nNo.\nAI does zero availability computation.\nLLMs cannot evaluate millions of rate combinations or room contracts.\nWhat AI does is:\ninterpret user intent break it into multiple sub-questions call booking APIs repeatedly ask for more data until it can confidently answer This means AI replaces the user, not the travel stack.\nThe backend still does all the work ‚Äî just 5‚Äì10√ó more of it.\nüîç Welcome to RAG: Retriever-Augmented Generation When an AI agent tries to answer:\n‚ÄúShow me hotels in Barcelona under ‚Ç¨150 with a sea view and free cancellation.‚Äù\nIt doesn‚Äôt send one availability request.\nIt sends many:\nretrieve price buckets retrieve cancellation policies retrieve board types retrieve room-level metadata check availability again to confirm re-filter after user clarification repeat steps for multiple suppliers This is the RAG pipeline:\nRetrieve ‚Üí Reason ‚Üí Retrieve ‚Üí Answer.\nüìà The impact: A search that used to cost $0.01 now easily becomes $0.05‚Äì$0.10.\nMultiply by thousands of RAG calls per hour, per OTA, per partner‚Ä¶\nAnd suddenly your infrastructure cost explodes ‚Äî not the AI company‚Äôs.\nü§ñ AI Has Lowered the Bar for Crawlers Even if your platform is not using RAG internally, you‚Äôre still affected.\nWhy?\nBecause AI tools make screen scraping and crawling far easier.\nCompetitor intelligence startups can now spin up:\nprice crawlers rate-parity compliance bots availability testers optimization scripts content diff bots ‚Ä¶with almost no engineering.\nBefore LLMs, building these tools required real engineering talent.\nNow:\nGPT writes your crawler Cloudflare proxies it A $5/month server runs it A new ‚Äúcompetitive intelligence company‚Äù appears every week Your search API becomes the data source for dozens of third parties ‚Äî\nmany of whom you will never know about.\nResult: Your infrastructure is working harder than ever,\nfor people who are not even your customers.\nüí£ The Hidden Cost Explosion in Travel Distribution Let‚Äôs summarize the problem:\nAI agents multiply search traffic.\nOne user ‚Üí many backend calls.\nCrawlers and scrapers multiply traffic further.\nZero friction to build, cheap to run.\nLegacy booking engines were not designed for multi-agent LLM usage.\nThey choke under unpredictable load.\nSearch is already the most expensive part of travel distribution.\nAI turns an expensive operation into a very expensive one.\nMost systems pay $50‚Äì$100 per million searches.\nWith RAG, that becomes $300‚Äì$700.\nWho pays?\nBedbanks. Consolidators. OTAs. Suppliers.\nEveryone except the AI companies.\nüßµ The New Problem AI Creates (That Nobody Is Talking About) Travel is one of the few industries where:\nresults depend on real-time data prices change constantly availability shifts instantly suppliers and contracts are complex merging/dedupe is computationally heavy LLM interfaces don‚Äôt simplify this.\nThey stress the infrastructure harder.\nAI creates a backend problem ‚Äî not a UI problem. And this is exactly why efficient, low-cost, ultra-fast availability engines will decide which travel companies survive.\nüìå Final Thought AI looks magical.\nBut someone is paying the price for the magic.\nToday, that someone is the bedbank, wholesaler, or OTA running the availability API.\nSearch cost is becoming the biggest hidden expense in travel.\nAI is accelerating it.\nAnd the industry isn‚Äôt prepared.\nIn future posts, I will explain:\nwhy AI-era search load increases 5‚Äì10√ó how to reduce cost per million search by 100√ó how to build RAG-friendly availability caches and why most travel systems will break under AI load Stay tuned.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe‚Äôve all seen the demos.\u003cbr\u003e\nA user chats with an AI agent and gets a perfectly planned vacation.\u003cbr\u003e\nThe interface is magical. Seamless. Effortless.\u003c/p\u003e\n\u003cp\u003eBut here‚Äôs the real question:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is happening behind the scenes ‚Äî and who is paying for it?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpoiler: \u003cem\u003eIt‚Äôs not the AI company. It‚Äôs you.\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-ai-is-not-replacing-software--its-replacing-the-user\"\u003eüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User\u003c/h2\u003e\n\u003cp\u003eThere is a growing misconception in travel tech:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúAI will replace the booking engines.‚Äù\u003c/p\u003e","title":"Why Are You Paying For Other People's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We‚Äôve all seen the demos.\nA user chats with an AI agent and gets a perfectly planned vacation.\nThe interface is magical. Seamless. Effortless.\nBut here‚Äôs the real question:\nWhat is happening behind the scenes ‚Äî and who is paying for it?\nSpoiler: It‚Äôs not the AI company. It‚Äôs you.\nüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User There is a growing misconception in travel tech:\n‚ÄúAI will replace the booking engines.‚Äù\nNo.\nAI does zero availability computation.\nLLMs cannot evaluate millions of rate combinations or room contracts.\nWhat AI does is:\ninterpret user intent break it into multiple sub-questions call booking APIs repeatedly ask for more data until it can confidently answer This means AI replaces the user, not the travel stack.\nThe backend still does all the work ‚Äî just 5‚Äì10√ó more of it.\nüîç Welcome to RAG: Retriever-Augmented Generation When an AI agent tries to answer:\n‚ÄúShow me hotels in Barcelona under ‚Ç¨150 with a sea view and free cancellation.‚Äù\nIt doesn‚Äôt send one availability request.\nIt sends many:\nretrieve price buckets retrieve cancellation policies retrieve board types retrieve room-level metadata check availability again to confirm re-filter after user clarification repeat steps for multiple suppliers This is the RAG pipeline:\nRetrieve ‚Üí Reason ‚Üí Retrieve ‚Üí Answer.\nüìà The impact: A search that used to cost $0.01 now easily becomes $0.05‚Äì$0.10.\nMultiply by thousands of RAG calls per hour, per OTA, per partner‚Ä¶\nAnd suddenly your infrastructure cost explodes ‚Äî not the AI company‚Äôs.\nü§ñ AI Has Lowered the Bar for Crawlers Even if your platform is not using RAG internally, you‚Äôre still affected.\nWhy?\nBecause AI tools make screen scraping and crawling far easier.\nCompetitor intelligence startups can now spin up:\nprice crawlers rate-parity compliance bots availability testers optimization scripts content diff bots ‚Ä¶with almost no engineering.\nBefore LLMs, building these tools required real engineering talent.\nNow:\nGPT writes your crawler Cloudflare proxies it A $5/month server runs it A new ‚Äúcompetitive intelligence company‚Äù appears every week Your search API becomes the data source for dozens of third parties ‚Äî\nmany of whom you will never know about.\nResult: Your infrastructure is working harder than ever,\nfor people who are not even your customers.\nüí£ The Hidden Cost Explosion in Travel Distribution Let‚Äôs summarize the problem:\nAI agents multiply search traffic.\nOne user ‚Üí many backend calls.\nCrawlers and scrapers multiply traffic further.\nZero friction to build, cheap to run.\nLegacy booking engines were not designed for multi-agent LLM usage.\nThey choke under unpredictable load.\nSearch is already the most expensive part of travel distribution.\nAI turns an expensive operation into a very expensive one.\nMost systems pay $50‚Äì$100 per million searches.\nWith RAG, that becomes $300‚Äì$700.\nWho pays?\nBedbanks. Consolidators. OTAs. Suppliers.\nEveryone except the AI companies.\nüßµ The New Problem AI Creates (That Nobody Is Talking About) Travel is one of the few industries where:\nresults depend on real-time data prices change constantly availability shifts instantly suppliers and contracts are complex merging/dedupe is computationally heavy LLM interfaces don‚Äôt simplify this.\nThey stress the infrastructure harder.\nAI creates a backend problem ‚Äî not a UI problem. And this is exactly why efficient, low-cost, ultra-fast availability engines will decide which travel companies survive.\nüìå Final Thought AI looks magical.\nBut someone is paying the price for the magic.\nToday, that someone is the bedbank, wholesaler, or OTA running the availability API.\nSearch cost is becoming the biggest hidden expense in travel.\nAI is accelerating it.\nAnd the industry isn‚Äôt prepared.\nIn future posts, I will explain:\nwhy AI-era search load increases 5‚Äì10√ó how to reduce cost per million search by 100√ó how to build RAG-friendly availability caches and why most travel systems will break under AI load Stay tuned.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe‚Äôve all seen the demos.\u003cbr\u003e\nA user chats with an AI agent and gets a perfectly planned vacation.\u003cbr\u003e\nThe interface is magical. Seamless. Effortless.\u003c/p\u003e\n\u003cp\u003eBut here‚Äôs the real question:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is happening behind the scenes ‚Äî and who is paying for it?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpoiler: \u003cem\u003eIt‚Äôs not the AI company. It‚Äôs you.\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-ai-is-not-replacing-software--its-replacing-the-user\"\u003eüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User\u003c/h2\u003e\n\u003cp\u003eThere is a growing misconception in travel tech:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúAI will replace the booking engines.‚Äù\u003c/p\u003e","title":"Why You Paying For Other People's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We‚Äôve all seen the demos.\nA user chats with an AI agent and gets a perfectly planned vacation.\nThe interface is magical. Seamless. Effortless.\nBut here‚Äôs the real question:\nWhat is happening behind the scenes ‚Äî and who is paying for it?\nSpoiler: It‚Äôs not the AI company. It‚Äôs you.\nüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User There is a growing misconception in travel tech:\n‚ÄúAI will replace the booking engines.‚Äù\nNo.\nAI does zero availability computation.\nLLMs cannot evaluate millions of rate combinations or room contracts.\nWhat AI does is:\ninterpret user intent break it into multiple sub-questions call booking APIs repeatedly ask for more data until it can confidently answer This means AI replaces the user, not the travel stack.\nThe backend still does all the work ‚Äî just 5‚Äì10√ó more of it.\nüîç Welcome to RAG: Retriever-Augmented Generation When an AI agent tries to answer:\n‚ÄúShow me hotels in Barcelona under ‚Ç¨150 with a sea view and free cancellation.‚Äù\nIt doesn‚Äôt send one availability request.\nIt sends many:\nretrieve price buckets retrieve cancellation policies retrieve board types retrieve room-level metadata check availability again to confirm re-filter after user clarification repeat steps for multiple suppliers This is the RAG pipeline:\nRetrieve ‚Üí Reason ‚Üí Retrieve ‚Üí Answer.\nüìà The impact: A search that used to cost $0.01 now easily becomes $0.05‚Äì$0.10.\nMultiply by thousands of RAG calls per hour, per OTA, per partner‚Ä¶\nAnd suddenly your infrastructure cost explodes ‚Äî not the AI company‚Äôs.\nü§ñ AI Has Lowered the Bar for Crawlers Even if your platform is not using RAG internally, you‚Äôre still affected.\nWhy?\nBecause AI tools make screen scraping and crawling far easier. Anybody can vibe code a crawler in a few hours.\nCompetitor intelligence startups can now spin up:\nprice crawlers rate-parity compliance bots availability testers optimization scripts content diff bots ‚Ä¶with almost no engineering.\nBefore LLMs, building these tools required real engineering talent.\nNow:\nGPT writes your crawler Cloudflare proxies it A $5/month server runs it A new ‚Äúcompetitive intelligence company‚Äù appears every week Your search API becomes the data source for dozens of third parties ‚Äî\nmany of whom you will never know about.\nResult: Your infrastructure is working harder than ever,\nfor people who are not even your customers.\nüí£ The Hidden Cost Explosion in Travel Distribution Let‚Äôs summarize the problem:\nAI agents multiply search traffic.\nOne user ‚Üí many backend calls.\nCrawlers and scrapers multiply traffic further.\nZero friction to build, cheap to run.\nLegacy booking engines were not designed for multi-agent LLM usage.\nThey choke under unpredictable load.\nSearch is already the most expensive part of travel distribution.\nAI turns an expensive operation into a very expensive one.\nMost systems pay $50‚Äì$100 per million searches.\nWith RAG, that becomes $300‚Äì$700.\nWho pays?\nBedbanks. Consolidators. OTAs. Suppliers.\nEveryone except the AI companies.\nüßµ The New Problem AI Creates (That Nobody Is Talking About) Travel is one of the few industries where:\nresults depend on real-time data prices change constantly availability shifts instantly suppliers and contracts are complex merging/dedupe is computationally heavy LLM interfaces don‚Äôt simplify this.\nThey stress the infrastructure harder.\nAI creates a backend problem ‚Äî not a UI problem. And this is exactly why efficient, low-cost, ultra-fast availability engines will decide which travel companies survive.\nüìå Final Thought AI looks magical.\nBut someone is paying the price for the magic.\nToday, that someone is the bedbank, wholesaler, or OTA running the availability API.\nSearch cost is becoming the biggest hidden expense in travel.\nAI is accelerating it.\nAnd the industry isn‚Äôt prepared.\nIn future posts, I will explain:\nwhy AI-era search load increases 5‚Äì10√ó how to reduce cost per million search by 100√ó how to build RAG-friendly availability caches and why most travel systems will break under AI load Stay tuned.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe‚Äôve all seen the demos.\u003cbr\u003e\nA user chats with an AI agent and gets a perfectly planned vacation.\u003cbr\u003e\nThe interface is magical. Seamless. Effortless.\u003c/p\u003e\n\u003cp\u003eBut here‚Äôs the real question:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is happening behind the scenes ‚Äî and who is paying for it?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpoiler: \u003cem\u003eIt‚Äôs not the AI company. It‚Äôs you.\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-ai-is-not-replacing-software--its-replacing-the-user\"\u003eüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User\u003c/h2\u003e\n\u003cp\u003eThere is a growing misconception in travel tech:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúAI will replace the booking engines.‚Äù\u003c/p\u003e","title":"Why You Paying For Other People's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We‚Äôve all seen the demos.\nA user chats with an AI agent and gets a perfectly planned vacation.\nThe interface is magical. Seamless. Effortless.\nBut here‚Äôs the real question:\nWhat is happening behind the scenes ‚Äî and who is paying for it?\nSpoiler: It‚Äôs not the AI company. It‚Äôs you.\nüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User There is a growing misconception in travel tech:\n‚ÄúAI will replace the booking engines.‚Äù\nNo.\nAI does zero availability computation.\nLLMs cannot evaluate millions of rate combinations or room contracts.\nWhat AI does is:\ninterpret user intent break it into multiple sub-questions call booking APIs repeatedly ask for more data until it can confidently answer This means AI replaces the user, not the travel stack.\nThe backend still does all the work ‚Äî just 5‚Äì10√ó more of it.\nüîç Welcome to RAG: Retriever-Augmented Generation When an AI agent tries to answer:\n‚ÄúShow me hotels in Barcelona under ‚Ç¨150 with a sea view and free cancellation.‚Äù\nIt doesn‚Äôt send one availability request.\nIt sends many:\nretrieve price buckets retrieve cancellation policies retrieve board types retrieve room-level metadata check availability again to confirm re-filter after user clarification repeat steps for multiple suppliers This is the RAG pipeline:\nRetrieve ‚Üí Reason ‚Üí Retrieve ‚Üí Answer.\nüìà The impact: A search that used to cost $0.01 now easily becomes $0.05‚Äì$0.10.\nMultiply by thousands of RAG calls per hour, per OTA, per partner‚Ä¶\nAnd suddenly your infrastructure cost explodes ‚Äî not the AI company‚Äôs.\nü§ñ AI Has Lowered the Bar for Crawlers Even if your platform is not using RAG internally, you‚Äôre still affected.\nWhy?\nBecause AI tools make screen scraping and crawling far easier. Anybody can vibe code a crawler in a few hours.\nCompetitor intelligence startups can now spin up:\nprice crawlers rate-parity compliance bots availability testers optimization scripts content diff bots ‚Ä¶with almost no engineering.\nBefore LLMs, building these tools required real engineering talent.\nNow:\nGPT writes your crawler Cloudflare proxies it A $5/month server runs it A new ‚Äúcompetitive intelligence company‚Äù appears every week Your search API becomes the data source for dozens of third parties ‚Äî\nmany of whom you will never know about.\nResult: Your infrastructure is working harder than ever,\nfor people who are not even your customers.\nüí£ The Hidden Cost Explosion in Travel Distribution Let‚Äôs summarize the problem:\nAI agents multiply search traffic.\nOne user ‚Üí many backend calls.\nCrawlers and scrapers multiply traffic further.\nZero friction to build, cheap to run.\nLegacy booking engines were not designed for multi-agent LLM usage.\nThey choke under unpredictable load.\nSearch is already the most expensive part of travel distribution.\nAI turns an expensive operation into a very expensive one.\nMost systems pay $50‚Äì$100 per million searches.\nWith RAG, that becomes $300‚Äì$700.\nWho pays?\nBedbanks. Consolidators. OTAs. Suppliers.\nEveryone except the AI companies.\nüßµ The New Problem AI Creates (That Nobody Is Talking About) Travel is one of the few industries where:\nresults depend on real-time data prices change constantly availability shifts instantly suppliers and contracts are complex merging/dedupe is computationally heavy LLM interfaces don‚Äôt simplify this.\nThey stress the infrastructure harder.\nAI creates a backend problem ‚Äî not a UI problem. And this is exactly why efficient, low-cost, ultra-fast availability engines will decide which travel companies survive.\nüìå Final Thought AI looks magical.\nBut someone is paying the price for the magic.\nToday, that someone is the bedbank, wholesaler, or OTA running the availability API.\nSearch cost is becoming the biggest hidden expense in travel.\nAI is accelerating it.\nAnd the industry isn‚Äôt prepared.\nIn future posts, I will explain:\nwhy AI-era search load increases 5‚Äì10√ó how to reduce cost per million search by 100√ó how to build RAG-friendly availability caches and why most travel systems will break under AI load Stay tuned.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe‚Äôve all seen the demos.\u003cbr\u003e\nA user chats with an AI agent and gets a perfectly planned vacation.\u003cbr\u003e\nThe interface is magical. Seamless. Effortless.\u003c/p\u003e\n\u003cp\u003eBut here‚Äôs the real question:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is happening behind the scenes ‚Äî and who is paying for it?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpoiler: \u003cem\u003eIt‚Äôs not the AI company. It‚Äôs you.\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-ai-is-not-replacing-software--its-replacing-the-user\"\u003eüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User\u003c/h2\u003e\n\u003cp\u003eThere is a growing misconception in travel tech:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúAI will replace the booking engines.‚Äù\u003c/p\u003e","title":"Why You Paying For Other People's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We‚Äôve all seen the demos.\nA user chats with an AI agent and gets a perfectly planned vacation.\nThe interface is magical. Seamless. Effortless.\nBut here‚Äôs the real question:\nWhat is happening behind the scenes ‚Äî and who is paying for it?\nSpoiler: It‚Äôs not the AI company. It‚Äôs you.\nüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User There is a growing misconception in travel tech:\n‚ÄúAI will replace the booking engines.‚Äù\nNo.\nAI does zero availability computation.\nLLMs cannot evaluate millions of rate combinations or room contracts.\nWhat AI does is:\ninterpret user intent break it into multiple sub-questions call booking APIs repeatedly ask for more data until it can confidently answer This means AI replaces the user, not the travel stack.\nThe backend still does all the work ‚Äî just 5‚Äì10√ó more of it.\nüîç Welcome to RAG: Retriever-Augmented Generation When an AI agent tries to answer:\n‚ÄúShow me hotels in Barcelona under ‚Ç¨150 with a sea view and free cancellation.‚Äù\nIt doesn‚Äôt send one availability request.\nIt sends many:\nretrieve price buckets retrieve cancellation policies retrieve board types retrieve room-level metadata check availability again to confirm re-filter after user clarification repeat steps for multiple suppliers This is the RAG pipeline:\nRetrieve ‚Üí Reason ‚Üí Retrieve ‚Üí Answer.\nüìà The impact: A search that used to cost $0.01 now easily becomes $0.05‚Äì$0.10.\nMultiply by thousands of RAG calls per hour, per OTA, per partner‚Ä¶\nAnd suddenly your infrastructure cost explodes ‚Äî not the AI company‚Äôs.\nü§ñ AI Has Lowered the Bar for Crawlers Even if your platform is not using RAG internally, you‚Äôre still affected.\nWhy?\nBecause AI tools make screen scraping and crawling far easier. Anybody can vibe code a crawler in a few hours.\nCompetitor intelligence startups can now spin up:\nprice crawlers rate-parity compliance bots availability testers optimization scripts content diff bots ‚Ä¶with almost no engineering.\nBefore LLMs, building these tools required real engineering talent.\nNow:\nGPT writes your crawler Cloudflare proxies it A $5/month server runs it A new ‚Äúcompetitive intelligence company‚Äù appears every week Your search API becomes the data source for dozens of third parties ‚Äî\nmany of whom you will never know about.\nResult: Your infrastructure is working harder than ever,\nfor people who are not even your customers.\nüí£ The Hidden Cost Explosion in Travel Distribution Let‚Äôs summarize the problem:\nAI agents multiply search traffic.\nOne user ‚Üí many backend calls.\nCrawlers and scrapers multiply traffic further.\nZero friction to build, cheap to run.\nLegacy booking engines were not designed for multi-agent LLM usage.\nThey choke under unpredictable load.\nSearch is already the most expensive part of travel distribution.\nAI turns an expensive operation into a very expensive one.\nMost systems pay $50‚Äì$100 per million searches.\nWith RAG, that becomes $300‚Äì$700.\nWho pays?\nBedbanks. Consolidators. OTAs. Suppliers.\nEveryone except the AI companies.\nüßµ The New Problem AI Creates (That Nobody Is Talking About) Travel is one of the few industries where:\nresults depend on real-time data prices change constantly availability shifts instantly suppliers and contracts are complex merging/dedupe is computationally heavy LLM interfaces don‚Äôt simplify this.\nThey stress the infrastructure harder.\nAI creates a backend problem ‚Äî not a UI problem. And this is exactly why efficient, low-cost, ultra-fast availability engines will decide which travel companies survive.\nüìå Final Thought AI looks magical.\nBut someone is paying the price for the magic.\nToday, that someone is the bedbank, wholesaler, or OTA running the availability API.\nSearch cost is becoming the biggest hidden expense in travel.\nAI is accelerating it.\nAnd the industry isn‚Äôt prepared.\nIn future posts, I will explain:\nwhy AI-era search load increases 5‚Äì10√ó how to reduce cost per million search by 100√ó how to build RAG-friendly availability caches and why most travel systems will break under AI load Stay tuned.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe‚Äôve all seen the demos.\u003cbr\u003e\nA user chats with an AI agent and gets a perfectly planned vacation.\u003cbr\u003e\nThe interface is magical. Seamless. Effortless.\u003c/p\u003e\n\u003cp\u003eBut here‚Äôs the real question:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is happening behind the scenes ‚Äî and who is paying for it?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpoiler: \u003cem\u003eIt‚Äôs not the AI company. It‚Äôs you.\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-ai-is-not-replacing-software--its-replacing-the-user\"\u003eüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User\u003c/h2\u003e\n\u003cp\u003eThere is a growing misconception in travel tech:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúAI will replace the booking engines.‚Äù\u003c/p\u003e","title":"Who is paying for other people's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"},{"content":"We‚Äôve all seen the demos.\nA user chats with an AI agent and gets a perfectly planned vacation.\nThe interface is magical. Seamless. Effortless.\nBut here‚Äôs the real question:\nWhat is happening behind the scenes ‚Äî and who is paying for it?\nSpoiler: It‚Äôs not the AI company. It‚Äôs you.\nüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User There is a growing misconception in travel tech:\n‚ÄúAI will replace the booking engines.‚Äù\nNo.\nAI does zero availability computation.\nLLMs cannot evaluate millions of rate combinations or room contracts.\nWhat AI does is:\ninterpret user intent break it into multiple sub-questions call booking APIs repeatedly ask for more data until it can confidently answer This means AI replaces the user, not the travel stack.\nThe backend still does all the work ‚Äî just 5‚Äì10√ó more of it.\nüîç Welcome to RAG: Retriever-Augmented Generation When an AI agent tries to answer:\n‚ÄúShow me hotels in Barcelona under ‚Ç¨150 with a sea view and free cancellation.‚Äù\nIt doesn‚Äôt send one availability request.\nIt sends many:\nretrieve price buckets retrieve cancellation policies retrieve board types retrieve room-level metadata check availability again to confirm re-filter after user clarification repeat steps for multiple suppliers This is the RAG pipeline:\nRetrieve ‚Üí Reason ‚Üí Retrieve ‚Üí Answer.\nüìà The impact: A search that used to cost $0.01 now easily becomes $0.05‚Äì$0.10.\nMultiply by thousands of RAG calls per hour, per OTA, per partner‚Ä¶\nAnd suddenly your infrastructure cost explodes ‚Äî not the AI company‚Äôs.\nü§ñ AI Has Lowered the Bar for Crawlers Even if your platform is not using RAG internally, you‚Äôre still affected.\nWhy?\nBecause AI tools make screen scraping and crawling far easier. Anybody can vibe code a crawler in a few hours.\nCompetitor intelligence startups can now spin up:\nprice crawlers rate-parity compliance bots availability testers optimization scripts content diff bots ‚Ä¶with almost no engineering.\nBefore LLMs, building these tools required real engineering talent.\nNow:\nGPT writes your crawler Cloudflare proxies it A $5/month server runs it A new ‚Äúcompetitive intelligence company‚Äù appears every week Your search API becomes the data source for dozens of third parties ‚Äî\nmany of whom you will never know about.\nResult: Your infrastructure is working harder than ever,\nfor people who are not even your customers.\nüí£ The Hidden Cost Explosion in Travel Distribution Let‚Äôs summarize the problem:\nAI agents multiply search traffic.\nOne user ‚Üí many backend calls.\nCrawlers and scrapers multiply traffic further.\nZero friction to build, cheap to run.\nLegacy booking engines were not designed for multi-agent LLM usage.\nThey choke under unpredictable load.\nSearch is already the most expensive part of travel distribution.\nAI turns an expensive operation into a very expensive one.\nMost systems pay $50‚Äì$100 per million searches.\nWith RAG, that becomes $300‚Äì$700.\nWho pays?\nBedbanks. Consolidators. OTAs. Suppliers.\nEveryone except the AI companies.\nüßµ The New Problem AI Creates (That Nobody Is Talking About) Travel is one of the few industries where:\nresults depend on real-time data prices change constantly availability shifts instantly suppliers and contracts are complex merging/dedupe is computationally heavy LLM interfaces don‚Äôt simplify this.\nThey stress the infrastructure harder.\nAI creates a backend problem ‚Äî not a UI problem. And this is exactly why efficient, low-cost, ultra-fast availability engines will decide which travel companies survive.\nüìå Final Thought AI looks magical.\nBut someone is paying the price for the magic.\nToday, that someone is the bedbank, wholesaler, or OTA running the availability API.\nSearch cost is becoming the biggest hidden expense in travel.\nAI is accelerating it.\nAnd the industry isn‚Äôt prepared.\nIn future posts, I will explain:\nwhy AI-era search load increases 5‚Äì10√ó how to reduce cost per million search by 100√ó how to build RAG-friendly availability caches and why most travel systems will break under AI load Stay tuned.\n","permalink":"//localhost:1313/posts/ai-cost/","summary":"\u003cp\u003eWe‚Äôve all seen the demos.\u003cbr\u003e\nA user chats with an AI agent and gets a perfectly planned vacation.\u003cbr\u003e\nThe interface is magical. Seamless. Effortless.\u003c/p\u003e\n\u003cp\u003eBut here‚Äôs the real question:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is happening behind the scenes ‚Äî and who is paying for it?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpoiler: \u003cem\u003eIt‚Äôs not the AI company. It‚Äôs you.\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"-ai-is-not-replacing-software--its-replacing-the-user\"\u003eüß† AI Is Not Replacing Software ‚Äî It‚Äôs Replacing the User\u003c/h2\u003e\n\u003cp\u003eThere is a growing misconception in travel tech:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e‚ÄúAI will replace the booking engines.‚Äù\u003c/p\u003e","title":"Who is Paying For Other People's AI?"},{"content":"If you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\nUnderstanding Look to Book Ratio The Look to Book ratio measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\n$$ \\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}} $$\nFor example, if a travel platform submits 1,000 hotel searches and receives 50 bookings in return, the Look to Book ratio is:\n$$ \\frac{1,000}{50} = 20 $$\nThis means 20 \u0026ldquo;looks\u0026rdquo; are required to generate one booking.\nWhen 1 Search Becomes 150 Requests However, this simple ratio hides a significant portion of the actual cost incurred by suppliers and API providers. Here‚Äôs why:\n1. Search Requests Are Amplified Internally Most travel APIs allow a single search request to query hundreds of hotels at once. Therefore, the number of actual hotel lookups is much higher than the number of incoming search requests:\n$$ \\text{Effective Look to Book} = \\\\ \\frac{ \\text{Search Requests} \\times \\text{Avg. Hotels per Request} }{ \\text{Bookings} } $$\nIf each search request contains 50 hotels, and 1,000 search requests are made, that‚Äôs 50,000 hotel lookups‚Äîjust for 50 bookings.\n2. Supplier Fan-Out Multiplies the Load Behind the scenes, many B2B travel platforms and wholesalers don\u0026rsquo;t own the hotel data‚Äîthey aggregate it from multiple suppliers. Each hotel search may fan out to several supplier APIs. A more realistic metric of internal load becomes:\n$$ \\text{Adjusted Look to Book} = \\\\ \\frac{ \\text{Search} \\times \\text{Hotels} \\times \\text{Suppliers} }{ \\text{Bookings} } $$\nLet‚Äôs say each hotel query is routed to 3 suppliers:\n$$ \\frac{1,000 \\times 50 \\times 3}{50} = 3,000 $$\nThat‚Äôs 3,000 backend supplier requests per booking‚Äîa 150x increase from the original ratio.\nWhy This Matters Infrastructure Costs: Each additional supplier call increases server load, bandwidth usage, and cloud compute costs. Rate Limiting \u0026amp; Throttling: High look-to-book ratios can lead to suppliers throttling your access or charging higher fees. Opportunity Cost: Excessive looking can exhaust shared resources unfairly. Bad performing consumers can deplete your supplier\u0026rsquo;s quota therefore you cannot sell to better performing consumers. Rethinking the Metric Instead of just reporting a simple Look to Book ratio, travel companies should also track:\nFan-out factor (supplier load per search) Effective backend look volume Cost per successful booking, including compute and supplier overhead Conclusion\nThe Look to Book ratio has long been a trusted benchmark in travel commerce, but its true cost lies beneath the surface. By accounting for the amplification through hotel-level fan-out and supplier-level propagation, travel platforms can better manage infrastructure, negotiate smarter contracts, and design systems that reward efficient traffic‚Äînot just high volumes.\n","permalink":"//localhost:1313/posts/hidden-cost-of-look-to-book/","summary":"\u003cp\u003eIf you‚Äôre paying per API call, every extra look costs you real dollars, but it goes way beyond that. Here is how you can calculate the actual cost of each look to optimize your traffic.\u003c/p\u003e\n\u003ch2 id=\"understanding-look-to-book-ratio\"\u003eUnderstanding Look to Book Ratio\u003c/h2\u003e\n\u003cp\u003eThe \u003cstrong\u003eLook to Book ratio\u003c/strong\u003e measures how many search requests (looks) are needed to generate a single confirmed booking. The basic formula is:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{Look to Book} = \\frac{\\text{Number of Searches}}{\\text{Number of Bookings}}\n$$\u003c/p\u003e","title":"The Hidden Cost of Look to Book"},{"content":"In 2020, I stepped into the role of Chief Technology Officer at Yolcu360, a rapidly-growing car rental platform in Turkey. My main challenge: modernize our tech stack from a fragile monolith into something scalable, reliable, and easy to integrate with partners around the globe.\nHere‚Äôs how that transformation unfolded‚Äîand what I learned.\nüõ†Ô∏è Breaking the Monolith: A Necessary Transition When I joined, Yolcu360‚Äôs core was a single, monolithic booking engine. While initially effective, this setup was now hampering our growth, slowing down deployments, and complicating integration.\nWe transitioned to a microservices architecture, primarily using:\nGo for critical services, chosen for performance and reliability. Kubernetes for service orchestration and deployment automation. Result?\nA 20-fold increase in throughput and dramatic improvement in reliability.\nüåç Implementing GIS Without the Mapping Pain Location accuracy is essential for car rental services. Instead of the typical and painful manual location-mapping process, we designed a GIS-based system that eliminated traditional mapping steps. It supported:\n14 languages seamlessly Real-time provider integration Accurate location matching across thousands of providers It cut onboarding time significantly and made the customer experience smoother and faster.\nü§ñ Launching a GPT-4 Powered Chatbot with Tool Use and UI Rendering One of our most exciting projects was developing an AI-driven chatbot. Our chatbot was far beyond basic Q\u0026amp;A‚Äîit was operational, interactive, and powered by GPT-4, with tool use and UI rendering capabilities which was not offered by OpenAI at the time. We built both tool use protocol, api and domain specific markup for UI rendering.\nStreaming LLM chains for real-time, interactive conversations. Ability to dynamically render forms and UI elements right inside the chat. Could invoke APIs directly, such as fetching real-time availability or completing bookings. This didn\u0026rsquo;t just automate interactions‚Äîit enhanced them.\nüìà Monitoring and Observability: Elastic + Prometheus Observability became a crucial part of our strategy. We shifted to a robust monitoring infrastructure using:\nElastic Stack for logs and metrics visualization. Prometheus for real-time monitoring and alerts. This combination gave us unprecedented visibility into performance, usage patterns, and errors, enabling proactive issue handling and continuous optimization.\nüöß Challenges and Learnings Reflecting back, here‚Äôs what stood out:\nTransitioning from a monolith demands careful planning. Gradual migrations, not big-bang rewrites, are key. Chatbots are powerful tools‚Äîif their capabilities align directly with real operational needs. Good observability doesn\u0026rsquo;t just help operations‚Äîit drives product decisions and customer success. This experience shaped much of my thinking for a new project. The ability to scale, integrate efficiently, and provide meaningful interactions is foundational‚Äînot just for car rentals, but for travel distribution at large.\nFacing similar challenges or interested in these approaches? Let‚Äôs chat.\n","permalink":"//localhost:1313/posts/yolcu-stack/","summary":"The journey from monolith to microservices, integrating GIS logic, and launching a GPT-4 powered chatbot.","title":"CTO Journal #2: Transforming a Car Rental Stack at Yolcu360"},{"content":"Yolcu360 is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\nThis is not a unique story. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\nMicroservices aren\u0026rsquo;t a silver bullet. They introduce complexity and require discipline. But if your product is growing and the pace of change matters, staying monolithic means you\u0026rsquo;re eventually paying a tax on every new feature.\nWhy Microservices Were Faster for Us Not every microservice outperforms its monolithic counterpart‚Äîbut breaking free from monolithic constraints opens up new avenues for scaling. We chose Go over Python for our concurrency-heavy services. That change alone allowed us to efficiently parallelize supplier requests. Introducing a centralized caching layer further reduced latency for high-volume lookups.\nMore importantly, services became \u0026ldquo;first-class citizens\u0026rdquo; in our Kubernetes environment. We could deploy, scale, and recover services independently‚Äîcritical when traffic spikes were unpredictable and needed instant elasticity.\nThe Result After completing the migration:\nThroughput increased 20x, enabling us to handle surges without degradation. Latency dropped significantly in key user flows, as shown in the chart. Integration time decreased, thanks to a new provider-mapping toolchain. Team velocity improved, as smaller, isolated services made onboarding and iteration faster. This wasn‚Äôt just a technical rewrite. It was a foundational shift that aligned our platform with the future of on-demand mobility infrastructure.\n","permalink":"//localhost:1313/posts/car-rental/","summary":"\u003cp\u003e\u003cstrong\u003eYolcu360\u003c/strong\u003e is the largest online car rental platform in Turkey. At its inception, the system was a traditional Django monolith‚Äîtypical for early-stage startups. When I stepped into the CTO role, we were facing performance bottlenecks and instability during traffic surges. For example an ad campaign with discounts in a popular tv show would degrade system performance and waste marketing budget.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis is not a unique story\u003c/strong\u003e. Many startups build MVPs quickly, and those temporary decisions become permanent as the business scales. Over time, technical debt compounds, and the monolith becomes a liability. You often hear ‚Äújust a refactor will fix it,‚Äù but in reality, few systems can sustain that illusion.\u003c/p\u003e","title":"Why We Moved to Microservices at Yolcu360"},{"content":"In 2017, I joined Metglobal, a major travel tech company with $350M+ in revenue, as Chief Technology Officer. My mission was clear: rebuild the core search and distribution engine to meet modern performance demands.\nWe were facing a bottleneck ‚Äî not just in raw throughput, but in flexibility. Every change took too long to propagate, and scaling wasn‚Äôt linear. Here\u0026rsquo;s what we did ‚Äî and what I learned.\nüöÄ Building a High-Performance Travel Search Engine We rebuilt the search platform with:\nGo as the service language Bigtable as our low-latency storage layer GRPC for fast inter-service communication The result?\nWe hit 300,000 responses per minute, all under 300ms, using fewer than 20 VMs.\nThis brought a 90% drop in server costs and almost linear scaling.\nIt was the most performant backend I‚Äôd ever deployed.\nüß† The Hard Parts (and What I\u0026rsquo;d Change) 1. Google\u0026rsquo;s Bigtable is brilliant ‚Äî but brutal without planning Bigtable offered the speed we needed, but schema design was critical. When patterns shifted, re-architecting keys was painful.\nIf I were to do it again, I\u0026rsquo;d:\nAdd a caching layer earlier (even with low latency storage) Abstract more query logic from table structure 2. Auto-SDK Generation Pays Off We built an OAPI-based system that generated SDKs for different teams. It improved adoption and consistency across products.\nToday, I‚Äôd go further:\nUse Buf Connect or OpenAPI + Typescript SDKs directly Treat internal APIs like products ‚Äî with versioning, docs, and test harnesses 3. Rule Engines Must Be Fast and Flexible We developed a product-level rule engine that adjusted prices and filters by hotel, room type, currency, provider, etc. It was powerful ‚Äî but tuning it took deep domain alignment.\nNow, I‚Äôd build this with:\nCode-defined rules + low-code overrides Integrated analytics to see the impact of each rule üìä Impact Beyond Tech The real win wasn‚Äôt just throughput.\nIt was how these decisions affected margin, market speed, and partner confidence.\nFast systems reduce retries, lower infrastructure bills, and increase the odds of conversion. In B2B travel, that matters more than ever.\nüß≠ What This Taught Me as a CTO Performance alone isn‚Äôt enough. Performance that aligns with business strategy is. Developer experience is key to scalability ‚Äî SDKs, APIs, and documentation aren‚Äôt luxuries. Building for throughput forces you to simplify smartly ‚Äî and abstract wisely. This project ultimately played a big role in the company‚Äôs acquisition by GoGlobal.\nToday, those same principles shape the foundation of a new project ‚Äî the travel distribution platform I‚Äôm now building from scratch.\nIf you\u0026rsquo;re scaling a B2B system ‚Äî in travel or otherwise ‚Äî let‚Äôs connect. I‚Äôve made all the mistakes already.\n","permalink":"//localhost:1313/posts/metglobal/","summary":"How we built a travel search engine that served 450,000 requests per minute under 300ms, and what I‚Äôd do differently today.","title":"Serving 450,000 requests per minute in HotelsPro"}]